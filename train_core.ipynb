{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 셋팅하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\begas\\Desktop\\Project\\SmartFarm\\1. DAT\\22_이엔티DB_신천농장_외부.csv\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------|\n",
    "# ----- Step 1. Settings ------------------------\n",
    "# -----------------------------------------------|\n",
    "os.chdir(\"C:/Users/begas/Desktop/Project/SmartFarm\")\n",
    "os.getcwd()\n",
    "\n",
    "home_path  = os.getcwd()\n",
    "data_path  = os.path.join(home_path,\"1. DAT\")\n",
    "save_path  = os.path.join(home_path,\"2. OUT\")\n",
    "model_path = os.path.join(home_path,\"3. MODEL\")\n",
    "\n",
    "data_files = os.path.join(data_path,os.listdir(data_path)[6])\n",
    "print(data_files)\n",
    "\n",
    "working_data = '20220916'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOAD_FN(data_files : os.path) -> pd.DataFrame :\n",
    "    '''\n",
    "    * 입력\n",
    "    data_files : raw 데이터 위치\n",
    "    \n",
    "    * 출력\n",
    "    raw_dat    : 데이터 프레임\n",
    "    '''\n",
    "    # 데이터 불러오기\n",
    "    raw_dat = pd.read_csv(data_files)\n",
    "    \n",
    "    # 데이터 변수명 변경\n",
    "    if 'sensingDt' in raw_dat.columns : raw_dat.rename(columns = {'sensingDt' : '시간'    }, inplace = True)\n",
    "    if 'nh3'       in raw_dat.columns : raw_dat.rename(columns = {'nh3'       : '암모니아'}, inplace = True)\n",
    "    if 'h2s'       in raw_dat.columns : raw_dat.rename(columns = {'h2s'       : '황화수소'}, inplace = True)\n",
    "    if 'tmp'       in raw_dat.columns : raw_dat.rename(columns = {'tmp'       : '온도'    }, inplace = True)\n",
    "    if 'hum'       in raw_dat.columns : raw_dat.rename(columns = {'hum'       : '습도'    }, inplace = True)\n",
    "    if 'voc'       in raw_dat.columns : raw_dat.rename(columns = {'voc'       : '환기팬'  }, inplace = True)\n",
    "        \n",
    "    # Time : date 변수로 변경\n",
    "    raw_dat['시간'] = pd.to_datetime(raw_dat['시간'])\n",
    "    \n",
    "    # 제거1. 암모니아와 황화수소가 두개의 변수일 경우 ppm으로 선택\n",
    "    if len([s for s in raw_dat.columns if '암모니아' in s]) > 1 : \n",
    "        del_cols = [s for s in raw_dat.columns if ('mV' in s) or ('(㎷)' in s)]\n",
    "        raw_dat = raw_dat.drop(columns = del_cols)\n",
    "\n",
    "    if len([s for s in raw_dat.columns if '황화수소' in s]) > 1 : \n",
    "        del_cols = [s for s in raw_dat.columns if ('mV' in s) or ('(㎷)' in s)]\n",
    "        raw_dat = raw_dat.drop(columns = del_cols)\n",
    "        \n",
    "    # 제거2. 변수명에 영어 포함시 영어 제거\n",
    "    cols_list = []\n",
    "    for cols in raw_dat.columns:\n",
    "        result = re.sub(\"[a-zA-Z]|[^\\w\\s]\", \"\", cols)\n",
    "        cols_list.append(result)\n",
    "    raw_dat.columns = cols_list\n",
    "    \n",
    "    # 제거3. 모든 값이 NA인 변수 제거\n",
    "    del_cols = raw_dat.columns[raw_dat.isna().mean() == 1]\n",
    "    if len(del_cols) > 0 :\n",
    "        print('모든 값이 NA인 변수 제거 :','/'.join(del_cols))\n",
    "        raw_dat = raw_dat.drop(columns = del_cols)\n",
    "\n",
    "    # 제거4. 단일값\n",
    "    col_unq_val = raw_dat.apply(lambda xx : len(xx.unique()), axis = 0)\n",
    "    del_cols    = raw_dat.columns[col_unq_val == 1]\n",
    "    if len(del_cols) > 0 :\n",
    "        print('모든 값이 단일값인 변수 제거 :','/'.join(del_cols))\n",
    "        raw_dat = raw_dat.drop(columns = del_cols) \n",
    "    \n",
    "    return raw_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 값이 단일값인 변수 제거 : 온도/환기팬\n"
     ]
    }
   ],
   "source": [
    "load_dat = LOAD_FN(data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 데이터 전처리 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PREPROCESS_FN(DAT : pd.DataFrame, time_grp : int) -> pd.DataFrame :\n",
    "    '''\n",
    "    * 입력\n",
    "    DAT              : LOAD_FN의 출력 데이터 프레임\n",
    "    time_grp         : 시간대별 통계량 요약 기준 단위(ex : if time_grp = 60, then 60분 단위로 통계량 요약)\n",
    "    \n",
    "    * 출력\n",
    "    final_summary_df : 데이터 프레임\n",
    "    '''\n",
    "    \n",
    "    print(\"Step1. 이상치를 허용범위 내로 보정\")\n",
    "    if '환기팬'       in DAT.columns : DAT.loc[( DAT['환기팬']       <   0 ) & (~DAT['환기팬'].isna())      ,'환기팬']       =   0\n",
    "    if '암모니아'     in DAT.columns : DAT.loc[( DAT['암모니아']     <   0 ) & (~DAT['암모니아'].isna())    ,'암모니아']     =   0\n",
    "    if '황화수소'     in DAT.columns : DAT.loc[( DAT['황화수소']     <   0 ) & (~DAT['황화수소'].isna())    ,'황화수소']     =   0\n",
    "    if '거품도포량'   in DAT.columns : DAT.loc[( DAT['거품도포량']   <   0 ) & (~DAT['거품도포량'].isna())  ,'거품도포량']   =   0\n",
    "    if '거품도포시간' in DAT.columns : DAT.loc[( DAT['거품도포시간'] <   0 ) & (~DAT['거품도포시간'].isna()),'거품도포시간'] =   0\n",
    "        \n",
    "    if '온도' in DAT.columns : DAT.loc[( DAT['온도'] >  50 ) & (~DAT['온도'].isna()),'온도'] =  50\n",
    "    if '온도' in DAT.columns : DAT.loc[( DAT['온도'] < -50 ) & (~DAT['온도'].isna()),'온도'] = -50\n",
    "    if '습도' in DAT.columns : DAT.loc[( DAT['습도'] > 100 ) & (~DAT['습도'].isna()),'습도'] = 100\n",
    "    if '습도' in DAT.columns : DAT.loc[( DAT['습도'] <   0 ) & (~DAT['습도'].isna()),'습도'] =   0\n",
    "        \n",
    "    print(\"Step2. 시간 변수를\", time_grp, \"분 단위로 변경\")\n",
    "    \n",
    "    def floor_dt(dt, time_grp = time_grp):\n",
    "        # how many secs have passed this day\n",
    "        nsecs = dt.hour*3600 + dt.minute*60 + dt.second + dt.microsecond*1e-6\n",
    "        delta = nsecs % (time_grp * 60)\n",
    "        return dt - datetime.timedelta(seconds=delta)\n",
    "    \n",
    "    DAT['시간'] = DAT['시간'].apply(floor_dt)  \n",
    "    \n",
    "    print(\"Step3. 시간별 요약통계량 데이터 생성\")\n",
    "    summary_cols = [s for s in DAT.columns if '시간' not in s]\n",
    "    mean_df = DAT.groupby('시간').apply(lambda xx : xx[summary_cols].mean(skipna = True)).reset_index(drop = False).rename(columns = {s:s+\"_mean\" for s in summary_cols})\n",
    "    min_df  = DAT.groupby('시간').apply(lambda xx : xx[summary_cols].min(skipna = True)).reset_index(drop = True).rename(columns = {s:s+\"_min\" for s in summary_cols})\n",
    "    max_df  = DAT.groupby('시간').apply(lambda xx : xx[summary_cols].max(skipna = True)).reset_index(drop = True).rename(columns = {s:s+\"_max\" for s in summary_cols})\n",
    "    std_df  = DAT.groupby('시간').apply(lambda xx : xx[summary_cols].std(skipna = True)).reset_index(drop = True).rename(columns = {s:s+\"_std\" for s in summary_cols})\n",
    "\n",
    "    # 요약 통계량 데이터 프레임 생성\n",
    "    summary_df = pd.concat([mean_df,min_df,max_df,std_df], axis = 1)\n",
    "    \n",
    "    print(\"Step4. 특정 시간대의 데이터가 비어있을 시 해당 시간대 생성\")\n",
    "    st_time = summary_df['시간'].iloc[0]\n",
    "    ed_time = summary_df['시간'].iloc[-1]\n",
    "    base_date_df = pd.DataFrame({'시간' : pd.date_range(st_time,ed_time, freq = 'H')})\n",
    "    n_missing_hour = len(set(set(base_date_df['시간'])) - set(set(summary_df['시간'])))\n",
    "    print(f'> {n_missing_hour}개의 시간대가 비어있습니다. 해당 시간대를 생성합니다.')\n",
    "    \n",
    "    print(\"Step5. 선형 보간법 적용\")\n",
    "    final_summary_df = pd.merge(base_date_df,summary_df, how = 'left', on = '시간')\n",
    "    final_summary_df.iloc[:,1:] = final_summary_df.iloc[:,1:].interpolate(method='linear')\n",
    "    \n",
    "    return final_summary_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1. 이상치를 허용범위 내로 보정\n",
      "Step2. 시간 변수를 60 분 단위로 변경\n",
      "Step3. 시간별 요약통계량 데이터 생성\n",
      "Step4. 특정 시간대의 데이터가 비어있을 시 해당 시간대 생성\n",
      "> 22개의 시간대가 비어있습니다. 해당 시간대를 생성합니다.\n",
      "Step5. 선형 보간법 적용\n"
     ]
    }
   ],
   "source": [
    "preprocess_result_df = PREPROCESS_FN(load_dat, time_grp = 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 - 분석용 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TRAIN_ANAL_DAT_FN(dat : pd.DataFrame, time_window : list) -> dict :\n",
    "    print(\"Step1. 목표변수: 암모니아, 황화수소 단위별 max 값\")\n",
    "    dat.rename(columns = {'암모니아_max' : 'y_암모니아', '황화수소_max' : 'y_황화수소','시간' : 'base_time'}, inplace = True)\n",
    "    dat.rename(columns = {s : 'x_' + s for s in dat.columns if ('time' not in s) and ('y_' not in s)}, inplace = True)\n",
    "    \n",
    "    print(\"Step2. 시차 변수 생성\")\n",
    "    old_dat = dat.copy()\n",
    "    new_dat = dat.copy()\n",
    "    \n",
    "    new_dat = new_dat[['base_time'] + [s for s in new_dat.columns if 'y_' in s]].copy()\n",
    "    new_dat.rename(columns = {'base_time' : 'predict_time'}, inplace = True)\n",
    "    \n",
    "    xvar_list = [s for s in old_dat.columns if 'x_' in s]\n",
    "    for w in time_window :\n",
    "        new_dat['base_time'] =\\\n",
    "        new_dat['predict_time'].apply(lambda xx : pd.date_range(end = xx, periods = w + 1, freq = 'H')[0])\n",
    "\n",
    "        new_dat = \\\n",
    "        pd.merge(new_dat, old_dat[['base_time'] + xvar_list].rename(columns = {s : s +'_'+str(w) for s in xvar_list})\n",
    "                , how = 'left'\n",
    "                , on  = 'base_time')\n",
    "        \n",
    "    # 시차 데이터로 인한 결측 제거\n",
    "    new_dat = new_dat.iloc[max(time_window):,:].reset_index(drop = True)\n",
    "    \n",
    "    print(\"Step3. 학습/검증 데이터 7:3으로 분할\")\n",
    "    train_dat = new_dat.iloc[:math.ceil(new_dat.shape[0] * 0.7),:].reset_index(drop = True)\n",
    "    test_dat  = new_dat.iloc[math.ceil(new_dat.shape[0] * 0.7):,:].reset_index(drop = True)\n",
    "    \n",
    "    print(\"Step4. 단일 값만 가지는 설명변수 제거\")\n",
    "    # 학습 데이터에서 단일값만 가지는 변수 제거\n",
    "    xvar_list = [s for s in train_dat.columns if 'x_' in s]\n",
    "    single_value_var_index = np.where(train_dat[xvar_list].apply(lambda xx : xx.nunique()) == 1)[0]\n",
    "    single_value_var_names = [xvar_list[s] for s in single_value_var_index]\n",
    "    xvar_list = list(set(xvar_list) - set(single_value_var_names))\n",
    "    xvar_list.sort()\n",
    "    \n",
    "    ret = dict({'train_dat' : train_dat, 'test_dat' : test_dat, 'x_var' : xvar_list})\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1. 목표변수: 암모니아, 황화수소 단위별 max 값\n",
      "Step2. 시차 변수 생성\n",
      "Step3. 학습/검증 데이터 7:3으로 분할\n",
      "Step4. 단일 값만 가지는 설명변수 제거\n"
     ]
    }
   ],
   "source": [
    "TRAIN_ANAL_DAT = TRAIN_ANAL_DAT_FN(preprocess_result_df, time_window = [12,24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\begas\\Anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:14: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import xgboost    as xgb\n",
    "import lightgbm   as lgb\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model    import LinearRegression\n",
    "from sklearn.ensemble        import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid, KFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MODELING_MLR(yvar_name : str, dat : pd.DataFrame) -> dict :\n",
    "    '''\n",
    "    * 입력\n",
    "    yvar_name : 종속변수 명\n",
    "    dat       : TRAIN_ANAL_DAT의 출력 데이터 프레임\n",
    "    \n",
    "    * 출력\n",
    "    dict      : 모델 및 모델 관련 정보들을 가지고 있는 딕셔너리\n",
    "    '''\n",
    "    print(f'Model Type is MLR')\n",
    "    \n",
    "    # 학습에 사용할 설명변수 명 지정\n",
    "    xvar_name = dat['x_var']\n",
    "\n",
    "    # 학습, 검증 데이터 준비\n",
    "    mlr_train_y = np.array(dat['train_dat'][yvar_name])\n",
    "    mlr_train_x = np.array(dat['train_dat'][xvar_name])\n",
    "    mlr_test_x  = np.array(dat['test_dat'][xvar_name])\n",
    "\n",
    "    # 회귀분석 Fitting\n",
    "    mlr_model = LinearRegression()\n",
    "    mlr_model.fit(X=mlr_train_x, y = mlr_train_y)\n",
    "\n",
    "    # 학습/검증데이터 예측\n",
    "    dat['train_dat']['pred'] = mlr_model.predict(X=mlr_train_x)\n",
    "    dat['test_dat']['pred'] = mlr_model.predict(X=mlr_test_x)\n",
    "    ret = dict({'model' : mlr_model,'model_name' : 'MLR' , \"yvar\" : yvar_name, \"xvar\" : xvar_name, \"train_res\" : dat['train_dat'], \"test_res\" : dat['test_dat']})\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLR_RESULT = MODELING_MLR(yvar_name = 'y_암모니아', dat = TRAIN_ANAL_DAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MODELING_RF(yvar_name : str, dat : pd.DataFrame) -> dict :\n",
    "    '''\n",
    "    * 입력\n",
    "    yvar_name : 종속변수 명\n",
    "    dat       : TRAIN_ANAL_DAT의 출력 데이터 프레임\n",
    "    \n",
    "    * 출력\n",
    "    dict      : 모델 및 모델 관련 정보들을 가지고 있는 딕셔너리\n",
    "    '''\n",
    "    print(f'Model Type is Random Forest')\n",
    "    \n",
    "    # 학습에 사용할 설명변수 명 지정\n",
    "    xvar_name = dat['x_var']\n",
    "\n",
    "    # 학습, 검증 데이터 준비\n",
    "    rf_train_y = np.array(dat['train_dat'][yvar_name])\n",
    "    rf_train_x = np.array(dat['train_dat'][xvar_name])\n",
    "    rf_test_x  = np.array(dat['test_dat'][xvar_name])\n",
    "    \n",
    "    # Random Forest Fitting\n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    rf_model.fit(X=rf_train_x, y = rf_train_y)\n",
    "    \n",
    "    # 학습/검증데이터 예측\n",
    "    dat['train_dat']['pred'] = rf_model.predict(rf_train_x)\n",
    "    dat['test_dat']['pred']  = rf_model.predict(rf_test_x)\n",
    "    ret = dict({'model' : rf_model,'model_name' : 'RF' , \"yvar\" : yvar_name, \"xvar\" : xvar_name, \"train_res\" : dat['train_dat'], \"test_res\" : dat['test_dat']})\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_RESULT = MODELING_RF(yvar_name = 'y_암모니아', dat = TRAIN_ANAL_DAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MODELING_XGB(yvar_name : str, dat : pd.DataFrame) -> dict :\n",
    "    '''\n",
    "    * 입력\n",
    "    yvar_name : 종속변수 명\n",
    "    dat       : TRAIN_ANAL_DAT의 출력 데이터 프레임\n",
    "    \n",
    "    * 출력\n",
    "    dict      : 모델 및 모델 관련 정보들을 가지고 있는 딕셔너리\n",
    "    '''\n",
    "    print(f'Model Type is XGBoost')\n",
    "    \n",
    "    # 학습에 사용할 설명변수 명 지정\n",
    "    xvar_name = dat['x_var']\n",
    "    \n",
    "    # XGBoost 학습 데이터 준비\n",
    "    train_d_mat = xgb.DMatrix(data = dat['train_dat'][xvar_name], label = dat['train_dat'][yvar_name])\n",
    "    \n",
    "    # Grid Search\n",
    "    params = {'max_depth':[5,7],\n",
    "              'min_child_weight':[1.0,3.0],\n",
    "              'colsample_bytree':[0.5,0.75]}\n",
    "    params_grid = pd.DataFrame(ParameterGrid(params))\n",
    "    \n",
    "    score_list           = []\n",
    "    num_boost_round_list = []\n",
    "    for params_idx, params in params_grid.iterrows() :\n",
    "        params_tmp  = {'max_depth'       : int(params['max_depth']),\n",
    "                       'min_child_weight': float(params['min_child_weight']),\n",
    "                       'colsample_bytree': float(params['colsample_bytree'])}\n",
    "        xgb_cv      = xgb.cv(dtrain = train_d_mat, params = params_tmp, num_boost_round = 200, nfold = 3, early_stopping_rounds = 10, maximize = 0, verbose_eval= 0, seed =1234)\n",
    "        num_boost_round_list.append(xgb_cv.shape[0])\n",
    "        score_list.append(xgb_cv['test-rmse-mean'].iloc[-1])\n",
    "    \n",
    "    # Find Best Parameter\n",
    "    params_grid['num_boost_round'] = num_boost_round_list\n",
    "    params_grid['score']           = score_list\n",
    "    best_params = params_grid.iloc[np.argmin(params_grid['score']),:]\n",
    "    xgb_train_params = {'max_depth'       : int(best_params['max_depth']),\n",
    "                        'min_child_weight': float(best_params['min_child_weight']),\n",
    "                        'colsample_bytree': float(best_params['colsample_bytree'])}\n",
    "    num_boost_round = int(best_params['num_boost_round'])    \n",
    "    \n",
    "    # XGBoost Fitting\n",
    "    xgb_model = xgb.train(dtrain = train_d_mat, params = xgb_train_params, num_boost_round = num_boost_round)\n",
    "    \n",
    "    # 학습/검증데이터 예측\n",
    "    dat['train_dat']['pred'] = xgb_model.predict(xgb.DMatrix(dat['train_dat'][dat['x_var']]))\n",
    "    dat['test_dat']['pred']  = xgb_model.predict(xgb.DMatrix(dat['test_dat'][dat['x_var']]))\n",
    "    \n",
    "    ret = dict({'model' : xgb_model,'model_name' : 'XGB' , \"yvar\" : yvar_name, \"xvar\" : xvar_name, \"train_res\" : dat['train_dat'], \"test_res\" : dat['test_dat']})\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_RESULT = MODELING_XGB(yvar_name = 'y_암모니아', dat = TRAIN_ANAL_DAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MODELING_LGB(yvar_name : str, dat : pd.DataFrame) -> dict :\n",
    "    '''\n",
    "    * 입력\n",
    "    yvar_name : 종속변수 명\n",
    "    dat       : TRAIN_ANAL_DAT의 출력 데이터 프레임\n",
    "    \n",
    "    * 출력\n",
    "    dict      : 모델 및 모델 관련 정보들을 가지고 있는 딕셔너리\n",
    "    '''\n",
    "    print(f'Model Type is LightGBM')\n",
    "    \n",
    "    # 학습에 사용할 설명변수 명 지정\n",
    "    xvar_name = dat['x_var']\n",
    "    \n",
    "    # LightGBM 학습 데이터 준비\n",
    "    train_d_mat = lgb.Dataset(data = dat['train_dat'][xvar_name], label = dat['train_dat'][yvar_name])\n",
    "    \n",
    "    # Grid Search\n",
    "    params = {'num_leaves' : [3,31],\n",
    "              'learning_rate' : [0.1],\n",
    "              'feature_fraction' : [1],\n",
    "              'bagging_fraction' : [1],\n",
    "              'max_bin' : [255]}\n",
    "    params_grid = pd.DataFrame(ParameterGrid(params))\n",
    "    score_list           = []\n",
    "    num_boost_round_list = []\n",
    "    for params_idx, params in params_grid.iterrows():\n",
    "        params_tmp = {'objective'        : 'regression',\n",
    "                      'boosting'         : \"gbdt\",\n",
    "                      'metric'           : 'rmse',\n",
    "                      'num_leaves'       : int(params['num_leaves']),\n",
    "                      'learning_rate'    : float(params['learning_rate']),\n",
    "                      'feature_fraction' : float(params['feature_fraction']),\n",
    "                      'bagging_fraction' : float(params['bagging_fraction']),\n",
    "                      'max_bin'          : int(params['max_bin'])}\n",
    "        lgb_cv     = lgb.cv(params = params_tmp, train_set = train_d_mat, num_boost_round = 200, nfold = 3, early_stopping_rounds = 10, verbose_eval= 0, seed =1234, stratified=False)\n",
    "        num_boost_round_list.append(len(lgb_cv['rmse-mean']))\n",
    "        score_list.append(lgb_cv['rmse-mean'][-1])\n",
    "    \n",
    "    # Find Best Parameter\n",
    "    params_grid['num_boost_round'] = num_boost_round_list\n",
    "    params_grid['score']           = score_list\n",
    "    best_params = params_grid.iloc[np.argmin(params_grid['score']),:]\n",
    "    \n",
    "    lgb_train_params = {'objective'        : 'regression',\n",
    "                        'boosting'         : \"gbdt\",\n",
    "                        'metric'           : 'rmse',\n",
    "                        'num_leaves'       : int(best_params['num_leaves']),\n",
    "                        'learning_rate'    : float(best_params['learning_rate']),\n",
    "                        'feature_fraction' : float(best_params['feature_fraction']),\n",
    "                        'bagging_fraction' : float(best_params['bagging_fraction']),\n",
    "                        'max_bin'          : int(best_params['max_bin'])}\n",
    "    num_boost_round = int(best_params['num_boost_round'])    \n",
    "    \n",
    "    # LightGBM Fitting\n",
    "    lgb_model     = lgb.train(params = params_tmp, train_set = train_d_mat, num_boost_round = num_boost_round)\n",
    "    \n",
    "    # 학습/검증데이터 예측\n",
    "    dat['train_dat']['pred'] = lgb_model.predict(dat['train_dat'][dat['x_var']])\n",
    "    dat['test_dat']['pred']  = lgb_model.predict(dat['test_dat'][dat['x_var']])\n",
    "\n",
    "    ret = dict({'model' : lgb_model,'model_name' : 'LGB' , \"yvar\" : yvar_name, \"xvar\" : xvar_name, \"train_res\" : dat['train_dat'], \"test_res\" : dat['test_dat']})\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGB_RESULT = MODELING_LGB(yvar_name = 'y_암모니아', dat = TRAIN_ANAL_DAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANN Model Class\n",
    "class MODELING_ANN():\n",
    "    def __init__(self, yvar_name : str, dat : pd.DataFrame) -> dict :\n",
    "        self.yvar_name = yvar_name\n",
    "        self.xvar_name = dat['x_var']\n",
    "        self.dat       = dat\n",
    "        self._preprocess()\n",
    "        self._fit()\n",
    "        self._pred()\n",
    "        self.ret = dict({'model' : self.ann.model,'model_name' : 'ANN' , \"yvar\" : self.yvar_name, \"xvar\" : self.xvar_name, \n",
    "                         \"train_res\" : self.dat['train_dat'], \"test_res\" : self.dat['test_dat'], \"scale_info\" : self.scale_info})\n",
    "\n",
    "    def _preprocess(self):\n",
    "        \n",
    "        # 전처리\n",
    "        self.train_x_array = np.array(self.dat['train_dat'][self.xvar_name])\n",
    "        self.train_y_array = np.array(self.dat['train_dat'][self.yvar_name])\n",
    "        self.test_x_array = np.array(self.dat['test_dat'][self.xvar_name])\n",
    "        self.test_y_array = np.array(self.dat['test_dat'][self.yvar_name])\n",
    "\n",
    "        # Min-Max Scaling\n",
    "        self.scale_info = pd.DataFrame({'xvar_name' : self.xvar_name, 'min' : np.apply_along_axis(min, 0, self.train_x_array), 'max' : np.apply_along_axis(max, 0, self.train_x_array)})\n",
    "        self.x_tr_scale = np.apply_along_axis(lambda xx : (xx  - self.scale_info['min'])/(self.scale_info['max'] - self.scale_info['min'] + 1e-10) ,1,self.train_x_array)\n",
    "        self.x_te_scale = np.apply_along_axis(lambda xx : (xx  - self.scale_info['min'])/(self.scale_info['max'] - self.scale_info['min'] + 1e-10) ,1,self.test_x_array)\n",
    "        self.x_te_scale[self.x_te_scale < 0] = 0\n",
    "        self.x_te_scale[self.x_te_scale > 1] = 1\n",
    "\n",
    "    def _fit(self):\n",
    "        # ANN Build\n",
    "        input_shape = self.train_x_array.shape[1] # input shape 설정\n",
    "        h_units     = [16,8]                  # 모델 Hidden Units 설정\n",
    "        self.ann = ann_model(input_shape,h_units) # 모델 Build\n",
    "        \n",
    "        # Early Stopping, Reduce Learning Rate, HIstory\n",
    "        EarlyStopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', mode = 'min', patience = 5, restore_best_weights=True, verbose = 0) \n",
    "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', mode = 'min', factor=0.5, patience=5, verbose=0, min_lr=1e-5)\n",
    "        history = tf.keras.callbacks.History()\n",
    "\n",
    "        self.ann.model.fit(self.x_tr_scale, self.train_y_array, validation_split = 0.3\n",
    "                                               , epochs = 100\n",
    "                                               , batch_size = 16\n",
    "                                               , callbacks = [EarlyStopping, reduce_lr, history]\n",
    "                                               , verbose = 0)\n",
    "    def _pred(self):\n",
    "        self.dat['train_dat']['pred'] = self.ann.model.predict(self.x_tr_scale)\n",
    "        self.dat['test_dat']['pred'] = self.ann.model.predict(self.x_te_scale)\n",
    "            \n",
    " ## ANN Model Class\n",
    "class ann_model():\n",
    "    def __init__(self, input_shape : int, h_units : list):\n",
    "        self.input_shape = input_shape\n",
    "        self.h_units     = h_units\n",
    "        self._build()\n",
    "        self._compile()\n",
    "\n",
    "    def _build(self):\n",
    "        input_layer  = tf.keras.Input(shape = self.input_shape, name = 'input_layer')\n",
    "        for idx,h in enumerate(self.h_units):\n",
    "            if idx == 0:\n",
    "                ann_layer = tf.keras.layers.Dense(h, activation = 'relu', name = f'ann_layer_{str(idx+1)}')(input_layer)\n",
    "            else :\n",
    "                ann_layer = tf.keras.layers.Dense(h, activation = 'relu', name = f'ann_layer_{str(idx+1)}')(ann_layer)\n",
    "        output_layer   = tf.keras.layers.Dense(1, activation = 'linear', name = 'output_layer')(ann_layer)\n",
    "        self.model = tf.keras.Model(inputs = input_layer, outputs = output_layer)\n",
    "\n",
    "    def _compile(self):\n",
    "        self.model.compile(optimizer = 'Adam', loss = 'mean_squared_error')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000168E7121798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000168E7121798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000168E7113AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000168E7113AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x00000168E72B8288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x00000168E72B8288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "ANN_RESULT = MODELING_ANN(yvar_name = 'y_암모니아', dat = TRAIN_ANAL_DAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM Model Class\n",
    "class MODELING_LSTM():\n",
    "    def __init__(self, yvar_name : str, dat : pd.DataFrame) -> dict :\n",
    "        self.yvar_name = yvar_name\n",
    "        self.xvar_name = dat['x_var']\n",
    "        self.dat       = dat\n",
    "        self._preprocess()\n",
    "        self._fit()\n",
    "        self._pred()\n",
    "        self.ret = dict({'model' : self.lstm.model,'model_name' : 'LSTM' , \"yvar\" : self.yvar_name, \"xvar\" : self.xvar_name, \n",
    "                         \"train_res\" : self.dat['train_dat'], \"test_res\" : self.dat['test_dat'], \"scale_info\" : self.scale_info})\n",
    "        \n",
    "        \n",
    "    def _preprocess(self):\n",
    "        # 전체 데이터 생성(시계열 설명변수 생성을 위해)\n",
    "        self.full_dat = pd.concat([self.dat['train_dat'],self.dat['test_dat']], ignore_index=True)\n",
    "\n",
    "        # LSTM 전처리 데이터 생성\n",
    "        self.nTimeStpes = 5\n",
    "        self.nInterval  = 1\n",
    "\n",
    "        # 디멘전 배치 사이즈\n",
    "        self.dim_batch = self.full_dat.shape[0] - (self.nInterval * (self.nTimeStpes - 1))\n",
    "\n",
    "        # 데이터 인덱스 생성\n",
    "        idx_list = []\n",
    "        for i in range(dim_batch):\n",
    "            idx_list.append(np.arange(start = i, stop = nInterval * (nTimeStpes - 1) + i + 1, step = nInterval))\n",
    "\n",
    "        # LSTM 시계열 데이터 생성    \n",
    "        x_array = []\n",
    "        y_array = []\n",
    "        date_df = []\n",
    "        for idx in range(len(idx_list)):\n",
    "            x_array.append(np.array(full_dat[xvar_name].iloc[idx_list[idx]]))\n",
    "            y_array.append(np.array(full_dat[yvar_name].iloc[idx_list[idx]]))\n",
    "            date_df.append(full_dat['predict_time'].iloc[idx_list[idx]].max())\n",
    "\n",
    "        self.x_array = np.array(x_array)\n",
    "        self.y_array = np.array(y_array)\n",
    "        self.date_df = np.array(date_df)    \n",
    "\n",
    "        # 학습, 검증 데이터로 다시 나누기\n",
    "        self.test_start_date = dat['test_dat']['predict_time'].iloc[0]\n",
    "\n",
    "        self.train_x_array = self.x_array[self.date_df<self.test_start_date,:,:]\n",
    "        self.train_y_array = self.y_array[self.date_df<self.test_start_date,:]\n",
    "\n",
    "        self.test_x_array = self.x_array[self.date_df>=self.test_start_date,:,:]\n",
    "        self.test_y_array = self.y_array[self.date_df>=self.test_start_date,:]\n",
    "\n",
    "        # Min, Max Scaling\n",
    "        self.scale_info = pd.DataFrame({'xvar_name' : xvar_name, 'min' : np.apply_along_axis(min, 0, np.vstack(self.train_x_array)), 'max' : np.apply_along_axis(max, 0, np.vstack(self.train_x_array))})\n",
    "\n",
    "        # Min, Max Scaling\n",
    "        self.x_tr_scale = np.apply_along_axis(lambda xx : (xx - self.scale_info['min'])/(self.scale_info['max'] - self.scale_info['min'] + 1e-10), 2, self.train_x_array)\n",
    "        self.x_te_scale = np.apply_along_axis(lambda xx : (xx - self.scale_info['min'])/(self.scale_info['max'] - self.scale_info['min'] + 1e-10), 2, self.test_x_array)\n",
    "        self.x_te_scale[self.x_te_scale<0] = 0\n",
    "        self.x_te_scale[self.x_te_scale>1] = 1\n",
    "\n",
    "            \n",
    "    def _fit(self):\n",
    "        # LSTM Build\n",
    "        self.input_shape = (self.nTimeStpes, self.train_x_array.shape[2]) # input shape 설정\n",
    "        self.h_units     = [4]                  # 모델 Hidden Units 설정\n",
    "        self.lstm = lstm_model(input_shape,h_units) # 모델 Build\n",
    "\n",
    "        # Early Stopping, Reduce Learning Rate, HIstory\n",
    "        EarlyStopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', mode = 'min', patience = 5, restore_best_weights=True, verbose = 0) \n",
    "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', mode = 'min', factor=0.5, patience=5, verbose=0, min_lr=1e-5)\n",
    "        history = tf.keras.callbacks.History()\n",
    "\n",
    "        self.lstm.model.fit(self.x_tr_scale, self.train_y_array, validation_split = 0.3\n",
    "                                               , epochs = 100\n",
    "                                               , batch_size = 16\n",
    "                                               , callbacks = [EarlyStopping, reduce_lr, history]\n",
    "                                               , verbose = 0)\n",
    "        \n",
    "\n",
    "    def _pred(self):\n",
    "        self.tr_pred = self.lstm.model.predict(self.x_tr_scale)\n",
    "        self.te_pred = self.lstm.model.predict(self.x_te_scale)\n",
    "\n",
    "        self.tr_pred = np.array([self.tr_pred[s][-1] for s in range(len(self.tr_pred))])\n",
    "        self.tr_pred = np.vstack([np.zeros(self.nInterval * (self.nTimeStpes - 1)).reshape(-1,1),self.tr_pred])\n",
    "        self.te_pred = np.array([self.te_pred[s][-1] for s in range(len(self.te_pred))])\n",
    "        \n",
    "        \n",
    "## LSTM Model Class\n",
    "class lstm_model():\n",
    "    def __init__(self, input_shape : tuple, h_units : list):\n",
    "        self.input_shape = input_shape\n",
    "        self.h_units     = h_units\n",
    "        self._build()\n",
    "        self._compile()\n",
    "\n",
    "    def _build(self):\n",
    "        input_layer = tf.keras.Input(shape = self.input_shape, name = 'input_layer')\n",
    "        for idx,h in enumerate(self.h_units):\n",
    "            if idx == 0:\n",
    "                lstm_layer = tf.keras.layers.LSTM(h, return_sequences=True, name = f'lstm_layer_{str(idx+1)}')(input_layer)\n",
    "            else :\n",
    "                lstm_layer = tf.keras.layers.LSTM(h, return_sequences=True, name = f'lstm_layer_{str(idx+1)}')(lstm_layer)\n",
    "        output_layer = tf.keras.layers.Dense(1, activation = 'linear', name = 'output_layer')(lstm_layer)  \n",
    "        self.model = tf.keras.Model(inputs = input_layer, outputs = output_layer)\n",
    "\n",
    "    def _compile(self):\n",
    "        self.model.compile(optimizer = 'Adam', loss = 'mean_squared_error')        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000168FC2F9E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000168FC2F9E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000168FD6ECA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000168FD6ECA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x00000168FFC208B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x00000168FFC208B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "8/8 [==============================] - 1s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "LSTM_RESULT = MODELING_LSTM(yvar_name = 'y_암모니아', dat = TRAIN_ANAL_DAT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
